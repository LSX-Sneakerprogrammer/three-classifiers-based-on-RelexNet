{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"relexnet (without modifier).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOgiffNv/a+al3MTmagSlR7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rq-8cdE6uSVj","executionInfo":{"status":"ok","timestamp":1622965249509,"user_tz":-480,"elapsed":46302,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"18a90f47-3659-498b-f949-93821d1cbbfa"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 160772 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.26-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPU2-Bjaw3if","executionInfo":{"status":"ok","timestamp":1622965254958,"user_tz":-480,"elapsed":510,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"2c10ddeb-6167-4f60-eeb0-d5a77c4bc379"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sun Jun  6 07:40:54 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bWU91uoOxa0F","executionInfo":{"status":"ok","timestamp":1622965255699,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["!mkdir -p drive\n","!google-drive-ocamlfuse -o nonempty drive\n","import os\n","import sys\n","os.chdir('drive')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"keypfrnLhTp5","executionInfo":{"status":"ok","timestamp":1622965260815,"user_tz":-480,"elapsed":4334,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["import nltk\n","from functools import lru_cache\n","import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter, defaultdict\n","from sklearn.preprocessing import OneHotEncoder\n","import torch.utils.data as Data\n","import torch.nn.functional as F\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import os\n","import glob\n","from sklearn.utils import shuffle\n","import copy"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knN_KDWrqNHX","executionInfo":{"status":"ok","timestamp":1622965260817,"user_tz":-480,"elapsed":7,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"7a551797-7fd0-4c68-94c8-5059378130f7"},"source":["if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    # print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print(torch.cuda.device_count())\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1\n","Device name: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCiTJxaMqlt-","executionInfo":{"status":"ok","timestamp":1622965260817,"user_tz":-480,"elapsed":6,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"27b3424d-9fef-42b5-f6a2-35a5a3ca223f"},"source":["!/opt/bin/nvidia-smi"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Sun Jun  6 07:40:59 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    26W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"by-7uhU7n1cs","executionInfo":{"status":"ok","timestamp":1622965260818,"user_tz":-480,"elapsed":4,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["class Preprocessor:\n","    def clean_text(self, text):\n","        # '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","        # make text lowercase\n","        text1 = text.lower()\n","        # remove square brackets\n","        text1 = re.sub('\\[.*?\\]', '', text1)\n","        #remove <>\n","        text1 = re.sub('<.*?>+', '', text1)\n","        text1 = re.sub('\\(.*?\\)', ' ', text1)\n","        text1 = re.sub('\\{.*?\\}', ' ', text1)\n","        # remove links\n","        text1 = re.sub('https?://\\S+|www\\.\\S+', ' ', text1)\n","        # remove punctuation\n","        text1 = re.sub('[%s]' % re.escape(string.punctuation), '', text1)\n","        # remove \\n\n","        # text = re.sub('\\n', '', text)\n","        # remove numbers\n","        text = re.sub('\\w*\\d\\w*', '', text)\n","        return text1\n","\n","    def __init__(self):\n","        self.stem = lru_cache(maxsize=10000)(nltk.stem.SnowballStemmer('english').stem)\n","        # self.stopwords = stopwords.words('english')\n","        self.tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize\n","\n","    def __call__(self, text):\n","        text1 = self.clean_text(text)\n","        tokens = self.tokenize(text1)\n","        # tokens = [token for token in tokens if token not in self.stopwords]\n","        # tokens = [self.stem(token) for token in tokens]\n","\n","        return tokens"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxP_rB3xoeLK","executionInfo":{"status":"ok","timestamp":1622965260818,"user_tz":-480,"elapsed":4,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["\n","class DataLoader:\n","    def __init__(self, data, batch_size, k, context_window = 1, preprocessor=Preprocessor(), enc_tokens = OneHotEncoder(), dev = 'cpu'):\n","        self.df = data\n","        self.dev = dev\n","        self.batch_size = batch_size\n","        self.context_window = context_window\n","        self.padding_length = 75\n","        self.fold_num = 10\n","        # self.padding_length = 192\n","        # self.first_sentence = 128\n","        # self.second_sentence = self.padding_length - self.first_sentence\n","\n","        self.apply_preprocessor(preprocessor)\n","        enc_tokens.fit(self.vocab)\n","        self.split(0.9, 0.1)\n","\n","        self.k_fold_partition(fold_num = self.fold_num) # Divide ten folds\n","        print(\"K Fold Partition Completed!\")\n","        train, validation = self.k_fold_split(k)\n","        trainX, trainY = self.build_training_data(enc_tokens, train)\n","        validationX, validationY = self.build_training_data(enc_tokens, validation)\n","        testX, testY = self.build_training_data(enc_tokens, self.test)\n","        print(f'Train Dataset Shape : {trainX.shape}')\n","        print(f'Validation Dataset Shape : {validationX.shape}')\n","        print(f'Test Dataset Shape : {testX.shape}')\n","        self.train_dataset = self.get_batch(trainX, trainY)\n","        self.validation_dataset = self.get_batch(validationX, validationY)\n","        self.test_dataset = self.get_batch(testX, testY)\n","\n","\n","    def get_batch(self, X, Y):\n","        dataset = Data.TensorDataset(X, Y)\n","        loader = Data.DataLoader(\n","            dataset=dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            # num_workers=2,\n","        )\n","        return loader\n","\n","\n","    def remove_low_frequency(self, list):\n","        new_list = []\n","        for x in list:\n","            if x in self.token_to_count.keys():\n","                new_list.append(x)\n","        return new_list\n","\n","\n","    def apply_preprocessor(self, preprocessor):\n","        self.df['tokens'] = [preprocessor(s) for s in self.df['sentence']]\n","        self.df['tokens'] = [x[:self.padding_length]  if len(x) > self.padding_length else x for x in self.df['tokens']]\n","\n","        # for index, row in self.df.iterrows():\n","        #     if len(row['tokens']) > self.max_length:\n","        #         row[toke]\n","\n","        self.token_to_count = Counter([x for l in self.df['tokens'] for x in l])\n","\n","        # tmp_token_to_count = self.token_to_count.copy()\n","        # for index, value in tmp_token_to_count.items():\n","        #     if value <= self.vocab_frequency:\n","        #         self.token_to_count.pop(index)\n","        # self.df['tokens'] = [self.remove_low_frequency(x) for x in self.df['tokens']]\n","        self.max_length = self.get_max_length()\n","        print(f'Max Length : {self.max_length}')\n","\n","        self.vocab = list([[term] for term in self.token_to_count.keys()])\n","\n","        print(f'Vocab Size : {len(self.vocab)}')\n","\n","    def get_max_length(self):\n","        max_length = 0\n","        for index, row in self.df.iterrows():\n","            token_list = [x for x in row['tokens']]\n","            tmp_length = len(token_list)\n","            if tmp_length > max_length:\n","                max_length = tmp_length\n","        return max_length\n","    def k_fold_partition(self, fold_num = 10):\n","        # index = list(range(len(self.df))) # total index\n","        batch_size = int(len(self.train) / fold_num) # the number of data for each fold\n","        remain_num = len(self.train) - batch_size * fold_num # the remain data after partition\n","        self.fold_data = []\n","        fold_batch_list = [] # Average the remaining data to the folds\n","        for fold in range(fold_num):\n","          if remain_num > 0:\n","            remain_num -= 1\n","            fold_batch_list.append(batch_size + 1)\n","          else:\n","            fold_batch_list.append(batch_size)\n","        fold_index = 0 # The starting position of each division data\n","        for fold in range(fold_num):\n","          fold_texts = [fold_index, fold_index + fold_batch_list[fold]]\n","          self.fold_data.append(fold_texts)\n","          fold_index = fold_index + fold_batch_list[fold]\n","\n","\n","    def k_fold_split(self, k):\n","        print(f'K : {k}, fold_data[k] : {self.fold_data[k]}')\n","        validation = self.train.iloc[self.fold_data[k][0]: self.fold_data[k][1]]\n","        train = self.train.iloc[0: self.fold_data[k][0]].append(self.train.iloc[self.fold_data[k][1]:])\n","        train_num = len(self.train) - (self.fold_data[k][1] - self.fold_data[k][0])\n","        if train_num % self.batch_size == 0:\n","            self.no_batch = train_num / self.batch_size\n","        else:\n","            self.no_batch = int(train_num / self.batch_size) + 1\n","        print(f' index : {self.no_batch}')\n","        return train, validation\n","\n","\n","    def split(self, train, test):\n","        index = int(train * len(self.df))\n","        self.train = self.df.iloc[0:index]\n","        self.test = self.df.iloc[index:]\n","\n","    def build_training_data(self, enc, df):\n","        X = []\n","        Y = []\n","        for index, row in df.iterrows():\n","            tmp = [enc.transform([[t]]).toarray()[0] for t in row['tokens']]\n","            if len(tmp) < self.max_length:\n","                pad_length = self.max_length - len(tmp)\n","                for i in range(pad_length):\n","                    tmp.append(np.zeros(len(self.vocab)))\n","            X.append(tmp)\n","            Y.append(row['label'])\n","        X = np.array(X)\n","        Y = np.array(Y)\n","        X = torch.from_numpy(X).float()\n","        Y = torch.from_numpy(Y).long()\n","        print(f'Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : {X.shape}')\n","        print(f'Input Label Shape : {Y.shape}')\n","        return X, Y\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOBWpAvWrAdh","executionInfo":{"status":"ok","timestamp":1622965469187,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["class RelexNet(nn.Module):\n","    #Single step RNN.\n","    #input_size is char_vacab_size=26,hidden_size is number of hidden neurons，output_size is number of categories\n","    def __init__(self, vocabulary_size, len_seq, num_classes):\n","        super(RelexNet, self).__init__()\n","        self.num_classes = num_classes\n","        self.L = nn.Linear(vocabulary_size, num_classes)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.O = nn.Softmax(dim=1)\n","\n","    def forward(self, input, B_layer):\n","        # X.shape = (batch, seq_len, vocab_size)\n","        T = input.shape[1]\n","        batch = input.shape[0]\n","        predict_y = Variable(torch.zeros(batch, self.num_classes))\n","\n","        if B_layer is None:\n","            B_layer = Variable(torch.zeros(batch, self.num_classes)).cuda()\n","\n","        for t in range(T):\n","            tmp = input[:, t, :]\n","\n","            L_onestep = self.L(tmp)\n","            L_onestep = self.dropout(L_onestep)\n","            L_onestep = torch.sigmoid(L_onestep)\n","            # L_onestep = F.relu6(L_onestep)\n","           \n","            B_layer = torch.add(B_layer, L_onestep)\n","            # print(B_layer)\n","\n","            if self.num_classes == 1:\n","                O_layer[t] = F.sigmoid(B_layer)\n","            else:\n","                O_layer = self.O(B_layer)\n","\n","        return O_layer, B_layer\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"rE53K7W8sP4K","executionInfo":{"status":"ok","timestamp":1622965469188,"user_tz":-480,"elapsed":2,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}}},"source":["def train_model(dataset, model, optimizer, scheduler, num_epochs, dev):\n","    losses = []\n","    for epoch in range(num_epochs):\n","        # training mode\n","        # dataset.set_partition(dataset.train)\n","        model.train()\n","        total_train_loss = 0\n","        total_train_correct = 0\n","        count = 0\n","        for x, y in dataset.train_dataset:\n","            # for every batch in the training dataset perform one update step of the optimizer.\n","            state = None\n","            model.zero_grad()\n","            y_h, state = model(x.to(dev), state)\n","            loss = F.cross_entropy(y_h, y.to(dev))\n","            optimizer.zero_grad()\n","            # scheduler.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            total_train_loss += loss.item()\n","            total_train_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","            count += 1\n","        average_train_loss = total_train_loss / count\n","        average_train_accuracy = total_train_correct / count\n","        losses.append(average_train_loss)\n","        print('{} optim: {}'.format(epoch + 1, optimizer.param_groups[0]['lr']))\n","        # print('{} optim: {}'.format(epoch, optimizer.param_groups[0]['lr']))\n","        # print('{} scheduler: {}'.format(epoch, scheduler.get_lr()[0]))\n","\n","        # validation mode\n","        model.eval()\n","        total_valid_loss = 0\n","        total_valid_correct = 0\n","\n","        count = 0\n","        for x, y in dataset.test_dataset:\n","            state = None\n","            y_h, state = model(x.to(dev), state)\n","            loss = F.cross_entropy(y_h, y.to(dev))\n","\n","            total_valid_loss += loss.item()\n","            total_valid_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","            count += 1\n","        average_valid_loss = total_valid_loss / count\n","        losses.append((average_train_loss, average_valid_loss))\n","        average_valid_accuracy = total_valid_correct / count\n","        print(f'epoch {epoch + 1} accuracies: \\t train: {average_train_accuracy}\\t valid: {average_valid_accuracy} loss: {average_train_loss}\\t')\n","\n","    # test mode\n","    # dataset.set_partition(dataset.test)\n","    model.eval()\n","    total_test_correct = 0\n","    count = 0\n","    for x, y in dataset.test_dataset:\n","        state = None\n","        y_h, state = model(x.to(dev), state)\n","        total_test_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","        count += 1\n","    average_test_accuracy = total_test_correct / count\n","\n","    print(f'test accuracy {average_test_accuracy}')\n","\n","    return losses, (average_train_accuracy, average_valid_accuracy, average_test_accuracy)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SbWvcYEurRyy","executionInfo":{"status":"ok","timestamp":1622965608058,"user_tz":-480,"elapsed":137467,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"df1766ad-1ee8-4b53-d55d-c80cc1056c69"},"source":["num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","k = 0\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","all_files = glob.glob(\"data/sentiment/*.csv\")\n","data = pd.concat((pd.read_csv(f, header=None, index_col=None) for f in all_files))\n","data.columns = ['sentence', 'label']\n","data = data.sample(frac=1, random_state=1)\n","dataset = DataLoader(data, k=k, batch_size = batch_size, context_window=context_window, dev=dev)\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","model = RelexNet(vocabulary_size=vocabulary_size, len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[8 * dataset.no_batch, 14 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","# torch.save(model, os.path.join('classifier.pth'))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","Max Length : 70\n","Vocab Size : 5309\n","K Fold Partition Completed!\n","K : 0, fold_data[k] : [0, 270]\n"," index : 76\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([2430, 70, 5309])\n","Input Label Shape : torch.Size([2430])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([270, 70, 5309])\n","Input Label Shape : torch.Size([270])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([300, 70, 5309])\n","Input Label Shape : torch.Size([300])\n","Train Dataset Shape : torch.Size([2430, 70, 5309])\n","Validation Dataset Shape : torch.Size([270, 70, 5309])\n","Test Dataset Shape : torch.Size([300, 70, 5309])\n","Data Ready!\n","1 optim: 0.01\n","epoch 1 accuracies: \t train: 0.5544407963752747\t valid: 0.6104166507720947 loss: 0.6827591767436579\t\n","2 optim: 0.01\n","epoch 2 accuracies: \t train: 0.7395833134651184\t valid: 0.659375011920929 loss: 0.6386701150944358\t\n","3 optim: 0.01\n","epoch 3 accuracies: \t train: 0.7809210419654846\t valid: 0.6979166865348816 loss: 0.6112252701269952\t\n","4 optim: 0.01\n","epoch 4 accuracies: \t train: 0.7851425409317017\t valid: 0.7416666746139526 loss: 0.5913872412945095\t\n","5 optim: 0.01\n","epoch 5 accuracies: \t train: 0.8457236886024475\t valid: 0.7708333730697632 loss: 0.5737813750379964\t\n","6 optim: 0.01\n","epoch 6 accuracies: \t train: 0.8439144492149353\t valid: 0.7447916865348816 loss: 0.5600425985298658\t\n","7 optim: 0.01\n","epoch 7 accuracies: \t train: 0.8609101176261902\t valid: 0.7479166388511658 loss: 0.5512882345601132\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.8436129689216614\t valid: 0.765625 loss: 0.5468661102809405\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.8818531036376953\t valid: 0.7593750357627869 loss: 0.5322186931183464\t\n","10 optim: 0.001\n","epoch 10 accuracies: \t train: 0.8790570497512817\t valid: 0.7458333373069763 loss: 0.5344611888653353\t\n","11 optim: 0.001\n","epoch 11 accuracies: \t train: 0.8802357912063599\t valid: 0.7562500238418579 loss: 0.5330145849209083\t\n","12 optim: 0.001\n","epoch 12 accuracies: \t train: 0.8811129331588745\t valid: 0.7718750238418579 loss: 0.5315401993299785\t\n","13 optim: 0.001\n","epoch 13 accuracies: \t train: 0.8893640637397766\t valid: 0.7645833492279053 loss: 0.5318781333534341\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.8844024538993835\t valid: 0.7562500238418579 loss: 0.5308214382905709\t\n","15 optim: 0.0001\n","epoch 15 accuracies: \t train: 0.8893640637397766\t valid: 0.7520833611488342 loss: 0.5302671460728896\t\n","16 optim: 0.0001\n","epoch 16 accuracies: \t train: 0.8847588300704956\t valid: 0.7572916746139526 loss: 0.5282387811886636\t\n","17 optim: 0.0001\n","epoch 17 accuracies: \t train: 0.8962445855140686\t valid: 0.7739583849906921 loss: 0.5276224064199548\t\n","18 optim: 0.0001\n","epoch 18 accuracies: \t train: 0.8879660367965698\t valid: 0.7614583373069763 loss: 0.5264907353802731\t\n","19 optim: 0.0001\n","epoch 19 accuracies: \t train: 0.8831140398979187\t valid: 0.768750011920929 loss: 0.5290262722655347\t\n","20 optim: 0.0001\n","epoch 20 accuracies: \t train: 0.8892818093299866\t valid: 0.7718750238418579 loss: 0.5249830825548423\t\n","test accuracy 0.7822916507720947\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bnFCCwTmWQF2"},"source":["The code below is used for test on stance detection dataset before, \n","which is not used at all"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-21dIPGEMbM-","executionInfo":{"elapsed":546559,"status":"ok","timestamp":1613487138602,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"},"user_tz":-480},"outputId":"1d71f3ee-0276-454f-937e-f25a29f10e12"},"source":["# Abortion\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","# root_path = 'data/stance'\n","# files = ['abortion', 'gayRights', 'marijuana', 'obama']\n","# label = {'abortion' : 0, 'gayRights' : 1, 'marijuana' : 2, 'obama' : 3}\n","# list_sentence = []\n","# list_label = []\n","# for file in files:\n","#     path = os.path.join('data', 'stance', file, '*.data')\n","#     print(path)\n","#     all_files = glob.glob(path)\n","#     # print(all_files)\n","#     for label_file in all_files:\n","#         f = open(label_file, 'r', encoding='UTF-8')\n","#         tmp = f.read()\n","#         list_sentence.append(tmp)\n","#         list_label.append(label[file])\n","\n","# dict = {'sentence': list_sentence, 'label': list_label}\n","# data = pd.DataFrame(dict)\n","# data = data.sample(frac=1, random_state=1)\n","\n","data = pd.read_csv(\"abortion.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=256)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","model = RelexNet(vocabulary_size=vocabulary_size, len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_abortion.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","1915\n","Max Length : 192\n","Vocab Size : 2093\n"," index : 24\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([766, 192, 2093])\n","Input Label Shape : torch.Size([766])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([574, 192, 2093])\n","Input Label Shape : torch.Size([574])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([575, 192, 2093])\n","Input Label Shape : torch.Size([575])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.5665798783302307\t valid: 0.5452961921691895\t valid loss: 0.6875425945798694\t precision: 0.3528748590755355\t recall: 0.5452961672473867\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.5754340887069702\t valid: 0.5452961921691895\t valid loss: 0.6877200154373455\t precision: 0.3528748590755355\t recall: 0.5452961672473867\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.5666667222976685\t valid: 0.5452961921691895\t valid loss: 0.6841082919246229\t precision: 0.3528748590755355\t recall: 0.5452961672473867\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.5757812857627869\t valid: 0.5452961921691895\t valid loss: 0.6823880428948054\t precision: 0.3528748590755355\t recall: 0.5452961672473867\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.5682291984558105\t valid: 0.5470383167266846\t valid loss: 0.6807021977386408\t precision: 0.35400225479143177\t recall: 0.5470383275261324\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.6080729365348816\t valid: 0.5470383167266846\t valid loss: 0.6806745127310736\t precision: 0.35400225479143177\t recall: 0.5470383275261324\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.6211805939674377\t valid: 0.5470383167266846\t valid loss: 0.6788738737019097\t precision: 0.35480225988700564\t recall: 0.5470383275261324\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.5798611640930176\t valid: 0.5696864128112793\t valid loss: 0.6775438525431663\t precision: 0.3797909407665505\t recall: 0.5696864111498258\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.7191840410232544\t valid: 0.5505226254463196\t valid loss: 0.6773637306295621\t precision: 0.358683314415437\t recall: 0.5505226480836237\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.5874131917953491\t valid: 0.5557491183280945\t valid loss: 0.6762349529756486\t precision: 0.36457142857142855\t recall: 0.5557491289198606\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.6720486879348755\t valid: 0.5662021040916443\t valid loss: 0.675967484868362\t precision: 0.373134328358209\t recall: 0.5662020905923345\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.6867188215255737\t valid: 0.5714285969734192\t valid loss: 0.6757829783270168\t precision: 0.38362573099415204\t recall: 0.5714285714285714\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.6881944537162781\t valid: 0.5714285969734192\t valid loss: 0.6757186911351174\t precision: 0.38006952491309387\t recall: 0.5714285714285714\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.6987847685813904\t valid: 0.5696864128112793\t valid loss: 0.6756110879810014\t precision: 0.38156359393232203\t recall: 0.5696864111498258\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.6983506679534912\t valid: 0.5696864128112793\t valid loss: 0.6755441865958404\t precision: 0.37891077636152953\t recall: 0.5696864111498258\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.6894965171813965\t valid: 0.5696864128112793\t valid loss: 0.6755380597783298\t precision: 0.37891077636152953\t recall: 0.5696864111498258\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.68359375\t valid: 0.5679442286491394\t valid loss: 0.6755278510291401\t precision: 0.3786295005807201\t recall: 0.5679442508710801\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.7024306058883667\t valid: 0.5679442286491394\t valid loss: 0.6755128330989166\t precision: 0.3786295005807201\t recall: 0.5679442508710801\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.6867188215255737\t valid: 0.5679442286491394\t valid loss: 0.6755057277891279\t precision: 0.3786295005807201\t recall: 0.5679442508710801\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.6837674379348755\t valid: 0.5679442286491394\t valid loss: 0.6754943922110136\t precision: 0.3786295005807201\t recall: 0.5679442508710801\t\n","test accuracy 0.582608699798584 precision 0.388631090487239 recall 0.5826086956521739\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXeaQG7IQlVF","executionInfo":{"elapsed":359231,"status":"ok","timestamp":1613487545549,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"},"user_tz":-480},"outputId":"c3e50c06-9b60-4b87-d4f2-83ec6d6ae3de"},"source":["# GayRights\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"gayRights.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size,len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_gayRights.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","1376\n","Max Length : 188\n","Vocab Size : 1629\n"," index : 18\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([550, 188, 1629])\n","Input Label Shape : torch.Size([550])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([412, 188, 1629])\n","Input Label Shape : torch.Size([412])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([414, 188, 1629])\n","Input Label Shape : torch.Size([414])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.4548611044883728\t valid: 0.6286407709121704\t valid loss: 0.66835132429322\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6616488872367201\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6611766978664305\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.6527777910232544\t valid: 0.6286407709121704\t valid loss: 0.6597741186908148\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.6302083134651184\t valid: 0.6286407709121704\t valid loss: 0.659259197492044\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.6302083134651184\t valid: 0.6286407709121704\t valid loss: 0.6583131135812084\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.6377314925193787\t valid: 0.6286407709121704\t valid loss: 0.6579086277091387\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6575866454434627\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6560648199424003\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.6469907164573669\t valid: 0.6286407709121704\t valid loss: 0.6558114522435132\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6555257628409608\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.6487268209457397\t valid: 0.6286407709121704\t valid loss: 0.6555229567931694\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.6377314925193787\t valid: 0.6286407709121704\t valid loss: 0.655472465582843\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.6302083134651184\t valid: 0.6286407709121704\t valid loss: 0.6553711560600012\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.6261574029922485\t valid: 0.6286407709121704\t valid loss: 0.6555017338505069\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.6394675970077515\t valid: 0.6286407709121704\t valid loss: 0.6555193325558912\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.6394675970077515\t valid: 0.6286407709121704\t valid loss: 0.6555257474334495\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.6394675970077515\t valid: 0.6286407709121704\t valid loss: 0.6555295914266873\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.6377314925193787\t valid: 0.6286407709121704\t valid loss: 0.6555189880207904\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.6469907164573669\t valid: 0.6286407709121704\t valid loss: 0.6555152492760454\t precision: 0.3859910581222057\t recall: 0.6286407766990292\t\n","test accuracy 0.6473429799079895 precision 0.39296187683284456 recall 0.6473429951690821\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZeMZ24XeFJd","executionInfo":{"elapsed":136282,"status":"ok","timestamp":1613487779476,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"},"user_tz":-480},"outputId":"45553b28-8e18-487a-f198-fe159ed5041a"},"source":["# Marijuana\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"marijuana.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size,len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_marijuana.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","626\n","Max Length : 182\n","Vocab Size : 866\n"," index : 8\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([250, 182, 866])\n","Input Label Shape : torch.Size([250])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([187, 182, 866])\n","Input Label Shape : torch.Size([187])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([189, 182, 866])\n","Input Label Shape : torch.Size([189])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.7454928159713745\t valid: 0.7058823704719543\t valid loss: 0.606004017718973\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6059681193076353\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.7463942170143127\t valid: 0.7058823704719543\t valid loss: 0.6061013513070377\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.6062580284906581\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.7472956776618958\t valid: 0.7058823704719543\t valid loss: 0.606291853170344\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.7400841116905212\t valid: 0.7058823704719543\t valid loss: 0.606327984422286\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.6062931695723917\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.7400841116905212\t valid: 0.7058823704719543\t valid loss: 0.6062524063701935\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.7445913553237915\t valid: 0.7058823704719543\t valid loss: 0.60610560650494\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6058972591066105\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.60586218623554\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6058372874310948\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6058113103244394\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6057674964481496\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.6057390175401208\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.7445913553237915\t valid: 0.7058823704719543\t valid loss: 0.6057346565200683\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.7472956776618958\t valid: 0.7058823704719543\t valid loss: 0.605729996361197\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6057276061193191\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.7418870329856873\t valid: 0.7058823704719543\t valid loss: 0.6057234175383726\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.7418870329856873\t valid: 0.7058823704719543\t valid loss: 0.6057197595025129\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","test accuracy 0.6666666269302368 precision 0.4 recall 0.6666666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ul7Qg4IYgS9C","executionInfo":{"elapsed":232177,"status":"ok","timestamp":1613488319259,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"},"user_tz":-480},"outputId":"bd1ea312-a28b-43c3-d48b-3e7fea2c1067"},"source":["# Obama\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"obama.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_obama.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","985\n","Max Length : 180\n","Vocab Size : 1315\n"," index : 13\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([394, 180, 1315])\n","Input Label Shape : torch.Size([394])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([295, 180, 1315])\n","Input Label Shape : torch.Size([295])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([296, 180, 1315])\n","Input Label Shape : torch.Size([296])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.48125001788139343\t valid: 0.4644067883491516\t valid loss: 0.7405359450033155\t precision: 1.0\t recall: 0.46440677966101696\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.48894232511520386\t valid: 0.47457626461982727\t valid loss: 0.695393718905368\t precision: 0.5857740585774058\t recall: 0.4745762711864407\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.5177884697914124\t valid: 0.5355932116508484\t valid loss: 0.691359292854697\t precision: 0.3487858719646799\t recall: 0.535593220338983\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.5105769038200378\t valid: 0.5593219995498657\t valid loss: 0.6892868672387075\t precision: 0.36585365853658536\t recall: 0.559322033898305\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.6653845906257629\t valid: 0.5694915056228638\t valid loss: 0.6888899225299642\t precision: 0.5581395348837209\t recall: 0.5694915254237288\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.7394230961799622\t valid: 0.5932203531265259\t valid loss: 0.6868721770027937\t precision: 0.4257907542579075\t recall: 0.5932203389830508\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.6399039030075073\t valid: 0.5728813409805298\t valid loss: 0.6848554595042083\t precision: 0.378076062639821\t recall: 0.5728813559322034\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.6471154093742371\t valid: 0.5898305177688599\t valid loss: 0.6839403746491771\t precision: 0.3927765237020316\t recall: 0.5898305084745763\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.7153846621513367\t valid: 0.6169491410255432\t valid loss: 0.6840363435826059\t precision: 0.44717444717444715\t recall: 0.6169491525423729\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.7706730961799622\t valid: 0.5932203531265259\t valid loss: 0.6836885177482993\t precision: 0.45691906005221933\t recall: 0.5932203389830508\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.763942301273346\t valid: 0.6203389763832092\t valid loss: 0.6829791119543173\t precision: 0.4430992736077482\t recall: 0.6203389830508474\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.7629807591438293\t valid: 0.5999999642372131\t valid loss: 0.6826641622236219\t precision: 0.42650602409638555\t recall: 0.6\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.7514423727989197\t valid: 0.5999999642372131\t valid loss: 0.6825339438551563\t precision: 0.4244604316546763\t recall: 0.6\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.7346154451370239\t valid: 0.5966101884841919\t valid loss: 0.682293130179583\t precision: 0.4180522565320665\t recall: 0.5966101694915255\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.7091346383094788\t valid: 0.5932203531265259\t valid loss: 0.6821217217687833\t precision: 0.4156769596199525\t recall: 0.5932203389830508\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.71875\t valid: 0.5932203531265259\t valid loss: 0.6821251956083006\t precision: 0.4156769596199525\t recall: 0.5932203389830508\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.7322115898132324\t valid: 0.5932203531265259\t valid loss: 0.6821329177436182\t precision: 0.4156769596199525\t recall: 0.5932203389830508\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.7139423489570618\t valid: 0.5932203531265259\t valid loss: 0.6821275306960284\t precision: 0.4156769596199525\t recall: 0.5932203389830508\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.7096154689788818\t valid: 0.5932203531265259\t valid loss: 0.6821276179814743\t precision: 0.4156769596199525\t recall: 0.5932203389830508\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.7442308068275452\t valid: 0.5966101884841919\t valid loss: 0.6821275822186874\t precision: 0.4180522565320665\t recall: 0.5966101694915255\t\n","test accuracy 0.5304054021835327 precision 0.39746835443037976 recall 0.5304054054054054\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":526},"id":"vnSy7vADC0kK","executionInfo":{"elapsed":11573,"status":"error","timestamp":1612420266396,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"},"user_tz":-480},"outputId":"55951f28-644f-457c-8765-a8f3372b30ca"},"source":["num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","# root_path = 'data/stance'\n","# files = ['abortion', 'gayRights', 'marijuana', 'obama']\n","# label = {'abortion' : 0, 'gayRights' : 1, 'marijuana' : 2, 'obama' : 3}\n","# list_sentence = []\n","# list_label = []\n","# for file in files:\n","#     path = os.path.join('data', 'stance', file, '*.data')\n","#     print(path)\n","#     all_files = glob.glob(path)\n","#     # print(all_files)\n","#     for label_file in all_files:\n","#         f = open(label_file, 'r', encoding='UTF-8')\n","#         tmp = f.read()\n","#         list_sentence.append(tmp)\n","#         list_label.append(label[file])\n","\n","# dict = {'sentence': list_sentence, 'label': list_label}\n","# data = pd.DataFrame(dict)\n","# data = data.sample(frac=1, random_state=1)\n","\n","data = pd.read_csv(\"stance_dataset.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 4\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.005,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[3 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.9, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier2.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","4902\n","Max Length : 189\n","Vocab Size : 3949\n","Modifier Size : 43\n","Negative Modifier Size : 29\n"," index : 62\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-ff746ca97aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# data = shuffle(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m# print(f'Data Size: {dataset.df.size()}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Ready!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, batch_size, context_window, preprocessor, enc_tokens, enc_modifiers, enc_negative_modifiers, modifier, dev)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(len(self.test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36mbuild_training_data\u001b[0;34m(self, enc, df)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_modifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_modifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],\n\u001b[0;32m--> 118\u001b[0;31m                                                      return_mask=True)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[0;34m(values, uniques, return_mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \"\"\"\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0muniques_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"cgJa4WP1rkU4"},"source":["torch.save(model, os.path.join('classifier.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdJa617xw7N3"},"source":["!pip install kora\n","from kora import console\n","console.start()  # and click link"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzzcdPQuRcyw"},"source":[""],"execution_count":null,"outputs":[]}]}