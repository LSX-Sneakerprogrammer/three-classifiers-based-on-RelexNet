{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"relexnet negative exper.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyML+Mo/bHFa8DBKWEHXxSje"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"Rq-8cdE6uSVj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622961609339,"user_tz":-480,"elapsed":65386,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"bb9f5b18-7d2a-447f-8056-472c65c777b7"},"source":["# Use in the google colab to connect the google cloud in order to get the dataset\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 160772 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.26-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.26-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPU2-Bjaw3if","executionInfo":{"status":"ok","timestamp":1622961615850,"user_tz":-480,"elapsed":707,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"4e286f4e-989d-4e35-dc7e-62c812076502"},"source":["# Use in google colab in order to get information about GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Jun  6 06:40:14 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bWU91uoOxa0F"},"source":["# Use in google colab in order to get the root path\n","!mkdir -p drive\n","!google-drive-ocamlfuse -o nonempty drive\n","import os\n","import sys\n","os.chdir('drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"keypfrnLhTp5"},"source":["import nltk\n","from functools import lru_cache\n","import re\n","import string\n","import numpy as np\n","import pandas as pd\n","import torch\n","from collections import Counter, defaultdict\n","from sklearn.preprocessing import OneHotEncoder\n","import torch.utils.data as Data\n","import torch.nn.functional as F\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import os\n","import glob\n","from sklearn.utils import shuffle\n","import copy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knN_KDWrqNHX","executionInfo":{"status":"ok","timestamp":1622961623782,"user_tz":-480,"elapsed":13,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"298c8005-6707-4196-dda6-25a68eb18d06"},"source":["# view gpu version\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    # print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print(torch.cuda.device_count())\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","Device name: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCiTJxaMqlt-","executionInfo":{"status":"ok","timestamp":1622961623782,"user_tz":-480,"elapsed":12,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"d5757160-5247-4d27-af42-f5084554c311"},"source":["!/opt/bin/nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Jun  6 06:40:22 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    26W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F36OHIItkZpx"},"source":["# Copyright@cjhutto https://github.com/cjhutto/vaderSentiment \n","class Modifiers:\n","    def __init__(self):\n","        C_INCR = 0.733\n","        B_INCR = 0.293\n","        B_DECR = -0.293\n","\n","        N_SCALAR = -0.74\n","        self.NEGATE = \\\n","            [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n","             \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n","             \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n","             \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n","             \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n","             \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n","             \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n","             \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n","\n","        # booster/dampener 'intensifiers' or 'degree adverbs'\n","        # \n","\n","        self.BOOSTER_DICT = \\\n","            {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR, \"completely\": B_INCR, \"considerably\": B_INCR,\n","             \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormously\": B_INCR,\n","             \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptionally\": B_INCR, \"extremely\": B_INCR,\n","             \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR,\n","             \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR,\n","             \"fucking\": B_INCR,\n","             \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR, \"incredibly\": B_INCR,\n","             \"intensely\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n","             \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n","             \"so\": B_INCR, \"substantially\": B_INCR,\n","             \"thoroughly\": B_INCR, \"totally\": B_INCR, \"tremendously\": B_INCR,\n","             \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utterly\": B_INCR,\n","             \"very\": B_INCR,\n","             \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n","             \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n","             \"less\": B_DECR, \"little\": B_DECR, \"marginally\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n","             \"scarcely\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n","             \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"by-7uhU7n1cs"},"source":["class Preprocessor:\n","    def clean_text(self, text):\n","        # '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","        # make text lowercase\n","        text1 = text.lower()\n","        # remove square brackets\n","        text1 = re.sub('\\[.*?\\]', '', text1)\n","        #remove <>\n","        text1 = re.sub('<.*?>+', '', text1)\n","        text1 = re.sub('\\(.*?\\)', ' ', text1)\n","        text1 = re.sub('\\{.*?\\}', ' ', text1)\n","        # remove links\n","        text1 = re.sub('https?://\\S+|www\\.\\S+', ' ', text1)\n","        # remove punctuation\n","        text1 = re.sub('[%s]' % re.escape(string.punctuation), '', text1)\n","        # remove \\n\n","        # text = re.sub('\\n', '', text)\n","        # remove numbers\n","        text = re.sub('\\w*\\d\\w*', '', text)\n","        return text1\n","\n","    def __init__(self):\n","        self.stem = lru_cache(maxsize=10000)(nltk.stem.SnowballStemmer('english').stem)\n","        # self.stopwords = stopwords.words('english')\n","        self.tokenize = nltk.tokenize.TreebankWordTokenizer().tokenize\n","\n","    def __call__(self, text):\n","        text1 = self.clean_text(text)\n","        tokens = self.tokenize(text1)\n","        # tokens = [token for token in tokens if token not in self.stopwords]\n","        # tokens = [self.stem(token) for token in tokens]\n","\n","        return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxP_rB3xoeLK"},"source":["\n","class DataLoader:\n","    def __init__(self, data, batch_size, k, context_window = 1, preprocessor=Preprocessor(), enc_tokens = OneHotEncoder(), enc_modifiers = OneHotEncoder(), modifier = Modifiers(), dev = 'cpu'):\n","        self.df = data # dataset\n","        self.dev = dev # choose \"gpu\" or \"cpu\"\n","        self.batch_size = batch_size # batch size\n","        self.context_window = context_window # context window size\n","        self.modifiers = modifier.BOOSTER_DICT.keys() # modifier lexicon\n","        self.enc_modifiers = enc_modifiers # one-hot encoder for modifiers\n","        # self.vocab_frequency = 5 # the lowest vocabulary frequency using in stance detection \n","        self.padding_length = 75 # padding length for network\n","        self.fold_num = 10 # 10-fold cross validation\n","        self.negatives = modifier.NEGATE\n","        self.count = 0 # count neighbor modifier\n","        # self.first_sentence = 128 # Intercept the first paragraph using in stance detection\n","        # self.second_sentence = self.padding_length - self.first_sentence # Intercept the last paragraph using in stance detection\n","\n","        self.apply_preprocessor(preprocessor) # clean text, tokenization\n","        enc_tokens.fit(self.vocab) # fit one-hot encoder for normal word lexicon\n","        self.enc_modifiers = enc_modifiers \n","        self.enc_modifiers.fit(self.modifier_id_to_fit) # fit one-hot encoder for modifiers\n","        self.split(0.9, 0.1) # Divide the training and test dataset\n","        self.k_fold_partition(fold_num = self.fold_num) # Divide ten folds\n","        print(\"K Fold Partition Completed!\")\n","        train, validation = self.k_fold_split(k)\n","        trainX, trainY = self.build_training_data(enc_tokens, train)\n","        validationX, validationY = self.build_training_data(enc_tokens, validation)\n","        testX, testY = self.build_training_data(enc_tokens, self.test)\n","        print(f'Train Dataset Shape : {trainX.shape}')\n","        print(f'Validation Dataset Shape : {validationX.shape}')\n","        print(f'Test Dataset Shape : {testX.shape}')\n","        self.train_dataset = self.get_batch(trainX, trainY)\n","        self.validation_dataset = self.get_batch(validationX, validationY)\n","        self.test_dataset = self.get_batch(testX, testY)\n","\n","\n","\n","    def get_batch(self, X, Y):\n","        dataset = Data.TensorDataset(X, Y)\n","        loader = Data.DataLoader(\n","            dataset=dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            # num_workers=2,\n","        )\n","        return loader\n","\n","\n","    def remove_low_frequency(self, list):\n","        new_list = []\n","        for x in list:\n","            if x in self.token_to_count.keys():\n","                new_list.append(x)\n","        return new_list\n","\n","\n","    def apply_preprocessor(self, preprocessor):\n","        self.df['tokens'] = [preprocessor(s) for s in self.df['sentence']]\n","        self.df['tokens'] = [x[:self.padding_length]  if len(x) > self.padding_length else x for x in self.df['tokens']]\n","        # for index, row in self.df.iterrows():\n","        #     if len(row['tokens']) > self.max_length:\n","        #         row[toke]\n","\n","        ######## Changed part for negatives ###########\n","        self.token_to_count = Counter([x for l in self.df['tokens'] for x in l if x not in self.modifiers and x not in self.negatives])\n","        self.modifiers_to_count = Counter([x for l in self.df['tokens'] for x in l if x in self.modifiers])\n","        self.negatives_to_count = Counter([x for l in self.df['tokens'] for x in l if x in self.negatives])\n","        ##############      End       #################\n","\n","        # tmp_token_to_count = self.token_to_count.copy()\n","        # for index, value in tmp_token_to_count.items():\n","        #     if value <= self.vocab_frequency:\n","        #         self.token_to_count.pop(index)\n","        # self.df['tokens'] = [self.remove_low_frequency(x) for x in self.df['tokens']]\n","        self.max_length = self.get_max_length()\n","        print(f'Max Length : {self.max_length}')\n","\n","        ######## Changed part for negatives ###########\n","        self.vocab = list([[term] for term in self.token_to_count.keys()])\n","        self.modifier_vocab = list([[term] for term in self.modifiers_to_count.keys()])\n","        self.negative_vocab = list([[term] for term in self.negatives_to_count.keys()])\n","        self.modifier_token_to_id = {self.modifier_vocab[i][0]: i + 1 for i in range(len(self.modifier_vocab))}\n","        self.modifier_id_to_fit = list([[term] for term in self.modifier_token_to_id.values()])\n","        ##############      End       #################\n","\n","        print(f'Vocab Size : {len(self.vocab)}')\n","        print(f'Modifier Size : {len(self.modifier_vocab)}')\n","        print(f'Negative Size : {len(self.negative_vocab)}')\n","        # print(self.token_to_count)\n","\n","    def get_max_length(self):\n","        max_length = 0\n","        for index, row in self.df.iterrows():\n","            ######## Changed part for negatives ###########\n","            token_list = [x for x in row['tokens'] if x not in self.modifiers and x not in self.negatives]\n","            ##############      End       #################\n","            tmp_length = len(token_list)\n","            if tmp_length > max_length:\n","                max_length = tmp_length\n","        return max_length\n","\n","    def k_fold_partition(self, fold_num = 10):\n","        batch_size = int(len(self.train) / fold_num) # the number of data for each fold\n","        remain_num = len(self.train) - batch_size * fold_num # the remain data after partition\n","        self.fold_data = []\n","        fold_batch_list = [] # Average the remaining data to the folds\n","        for fold in range(fold_num):\n","          if remain_num > 0:\n","            remain_num -= 1\n","            fold_batch_list.append(batch_size + 1)\n","          else:\n","            fold_batch_list.append(batch_size)\n","        fold_index = 0 # The starting position of each division data\n","        for fold in range(fold_num):\n","          fold_texts = [fold_index, fold_index + fold_batch_list[fold]]\n","          self.fold_data.append(fold_texts)\n","          fold_index = fold_index + fold_batch_list[fold]\n","\n","\n","    def k_fold_split(self, k):\n","        print(f'K : {k}, fold_data[k] : {self.fold_data[k]}')\n","        validation = self.train.iloc[self.fold_data[k][0]: self.fold_data[k][1]]\n","        train = self.train.iloc[0: self.fold_data[k][0]].append(self.train.iloc[self.fold_data[k][1]:])\n","        train_num = len(self.train) - (self.fold_data[k][1] - self.fold_data[k][0])\n","        if train_num % self.batch_size == 0:\n","            self.no_batch = train_num / self.batch_size\n","        else:\n","            self.no_batch = int(train_num / self.batch_size) + 1\n","        print(f' index : {self.no_batch}')\n","        return train, validation\n","\n","\n","\n","    def split(self, train, test):\n","        index = int(train * len(self.df))\n","        self.train = self.df.iloc[0:index]\n","        self.test = self.df.iloc[index:]\n","\n","    def build_training_data(self, enc, df):\n","        X = []\n","        Y = []\n","        for index, row in df.iterrows():\n","            # build modifiers and negative words\n","            np_modifier = np.zeros(len(row['tokens']))\n","            np_negative = np.zeros(len(row['tokens'])) # innitial negative ids\n","            for index, value in enumerate(row['tokens']):\n","                if value in self.modifiers:\n","                    # work for modifiers\n","                    id = self.modifier_token_to_id[value]\n","                    np_modifier[index] = -1\n","                    np_negative[index] = -1 # mark the negative word to delete later\n","                    for i in range(1, self.context_window + 1):\n","                        if index - i > 0:\n","                          # ensure the affected words is none negative words or modifiers\n","                          # if the the affected words is negative words or modifiers, move once \n","                          if row['tokens'][index - i] not in self.modifiers and row['tokens'][index - i] not in self.negatives:\n","                            np_modifier[index - i] = id\n","                          else:\n","                            if index - i - 1 > 0 and row['tokens'][index - i - 1] not in self.modifiers and row['tokens'][index - i - 1] not in self.negatives:\n","                              np_modifier[index - i - 1] = id\n","                        if index + i < len(row['tokens']):\n","                          # ensure the affected words is none negative words or modifiers\n","                          # if the the affected words is negative words or modifiers, move once \n","                          if row['tokens'][index + i] not in self.modifiers and row['tokens'][index + i] not in self.negatives:\n","                            np_modifier[index + i] = id\n","                          else:\n","                            if index + i + 1 < len(row['tokens']) and row['tokens'][index + i + 1] not in self.modifiers and row['tokens'][index + i + 1] not in self.negatives:\n","                              np_modifier[index + i + 1] = id\n","\n","                if value in self.negatives:\n","                    # work for negative words, similar with modifier method above\n","                    # The method uses np_negative[itself] = id - np_negative[itself] is to make\n","                    # sure that if there are two negative words, the switch will be twice\n","                    id = 1\n","                    np_negative[index] = -1\n","                    np_modifier[index] = -1 # mark the negative word to delete later\n","                    for i in range(1, self.context_window + 1):\n","                        if index - i > 0:\n","                          if row['tokens'][index - i] not in self.modifiers and row['tokens'][index - i] not in self.negatives:\n","                            np_negative[index - i] = id - np_negative[index - i]\n","                          else:\n","                            if index - i - 1 > 0 and row['tokens'][index - i - 1] not in self.modifiers and row['tokens'][index - i - 1] not in self.negatives:\n","                              np_negative[index - i - 1] = id - np_negative[index - i - 1]\n","                        if index + i < len(row['tokens']):\n","                          if row['tokens'][index + i] not in self.modifiers and row['tokens'][index + i] not in self.negatives:\n","                            np_negative[index + i] = id - np_negative[index + i]\n","                          else:\n","                            if index + i + 1 < len(row['tokens']) and row['tokens'][index + i + 1] not in self.modifiers and row['tokens'][index + i + 1] not in self.negatives:\n","                              np_negative[index + i + 1] = id - np_negative[index + i + 1]\n","                        \n","            # delete modifier and negative word\n","            deleted_index = [i for i in range(len(np_modifier)) if np_modifier[i] == -1]\n","            np_modifier = np.delete(np_modifier, deleted_index)\n","            np_negative = np.delete(np_negative, deleted_index)\n","\n","\n","            tmp = [enc.transform([[t]]).toarray()[0] for t in row['tokens'] if t not in self.modifiers and t not in self.negatives]\n","            for i in range(len(tmp)):\n","                # add the last as negative ids, the last before as modifier ids\n","                tmp[i] = np.append(tmp[i], np_modifier[i])\n","                tmp[i] = np.append(tmp[i], np_negative[i])\n","            if len(tmp) < self.max_length:\n","                pad_length = self.max_length - len(tmp)\n","                for i in range(pad_length):\n","                    tmp.append(np.append(np.zeros(len(self.vocab)), [-1, 0]))\n","            X.append(tmp)\n","            Y.append(row['label'])\n","        X = np.array(X)\n","        Y = np.array(Y)\n","        X = torch.from_numpy(X).float()\n","        Y = torch.from_numpy(Y).long()\n","        # print(f'Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : {X.shape}')\n","        # print(f'Input Label Shape : {Y.shape}')\n","        return X, Y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOBWpAvWrAdh"},"source":["class RelexNet(nn.Module):\n","    #Single step RNN.\n","    #input_size is char_vacab_size=26,hidden_size is number of hidden neurons，output_size is number of categories\n","    def __init__(self, vocabulary_size, modifier_size, len_seq, num_classes, enc):\n","        super(RelexNet, self).__init__()\n","        self.no_modifiers = np.zeros(modifier_size)\n","        self.num_classes = num_classes\n","        self.L = nn.Linear(vocabulary_size, num_classes)\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.M = nn.Linear(modifier_size, 1).cuda()\n","        # self.B = nn.Linear(len_seq, len_seq)\n","        self.enc = enc\n","        self.O = nn.Softmax(dim=1)\n","\n","    def handle_modifier(self, modifier_ids, batch):\n","        # modifier_id = modifier_ids.cpu()\n","        modifier = []\n","        influence_flag = []\n","        add = 0.5 * Variable(torch.ones(batch, 1))\n","        for i in range(batch):\n","            if modifier_ids[i] == 0:\n","                modifier.append(self.no_modifiers)\n","                influence_flag.append(0)\n","                add[i] = 1\n","            elif modifier_ids[i] == -1:\n","                modifier.append(self.no_modifiers)\n","                influence_flag.append(0)\n","                add[i] = 0\n","            else:\n","                # print(modifier_ids[i])\n","                modifier.append(self.enc.transform([[modifier_ids[i].cpu()]]).toarray()[0])\n","                influence_flag.append(1)\n","        modifier = torch.tensor(modifier).float().cuda()\n","        influence_flag = torch.tensor(influence_flag).float()\n","        influence_flag = influence_flag.reshape(-1, 1).cuda()\n","        add = add.cuda()\n","        return modifier, influence_flag, add\n","\n","\n","\n","    def forward(self, input, B_layer):\n","        # X.shape = (batch, seq_len, vocab_size)\n","        T = input.shape[1]\n","        batch = input.shape[0]\n","        predict_y = Variable(torch.zeros(batch, self.num_classes))\n","\n","        if B_layer is None:\n","            B_layer = Variable(torch.zeros(batch, self.num_classes)).cuda()\n","\n","        for t in range(T):\n","            tmp = input[:, t, :-2]\n","            modifier_ids = input[:, t, -2]\n","            negative_ids = input[:, t, -1]\n","            modifier, influence_flag, add = self.handle_modifier(modifier_ids, batch)\n","\n","            L_onestep = self.L(tmp)\n","            L_onestep = self.dropout(L_onestep)\n","            L_onestep = torch.sigmoid(L_onestep)\n","            L_onestep = F.relu6(L_onestep)\n","            # modifier_score = torch.sigmoid(self.M(modifier))\n","            modifier_score = torch.relu(self.M(modifier))\n","            modifier_score = modifier_score * influence_flag\n","            modifier_score = modifier_score + add\n","            S_layer = L_onestep * modifier_score\n","\n","\n","            # This step is to switch the score of negative and positive\n","            # Using matrix operation is to guarantee the speed\n","            new_negative_ids = negative_ids.reshape(-1,1).repeat(1,2)\n","            ES_layer = S_layer[:,[1,0]]\n","            N_layer = ES_layer * new_negative_ids + S_layer * (1 - new_negative_ids)\n","\n","            B_layer = torch.add(B_layer, N_layer)\n","            # print(B_layer)\n","\n","            if self.num_classes == 1:\n","                O_layer[t] = F.sigmoid(B_layer)\n","            else:\n","                O_layer = self.O(B_layer)\n","\n","        return O_layer, B_layer\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rE53K7W8sP4K"},"source":["def train_model(dataset, model, optimizer, scheduler, num_epochs, dev):\n","    losses = []\n","    for epoch in range(num_epochs):\n","        # training mode\n","        # dataset.set_partition(dataset.train)\n","        model.train()\n","        total_train_loss = 0\n","        total_train_correct = 0\n","        count = 0\n","        for x, y in dataset.train_dataset:\n","            # for every batch in the training dataset perform one update step of the optimizer.\n","            state = None\n","            model.zero_grad()\n","            y_h, state = model(x.to(dev), state)\n","            loss = F.cross_entropy(y_h, y.to(dev))\n","            optimizer.zero_grad()\n","            # scheduler.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            total_train_loss += loss.item()\n","            total_train_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","            count += 1\n","        average_train_loss = total_train_loss / count\n","        average_train_accuracy = total_train_correct / count\n","        print('{} optim: {}'.format(epoch + 1, optimizer.param_groups[0]['lr']))\n","\n","        # validation mode\n","        model.eval()\n","        total_valid_loss = 0\n","        total_valid_correct = 0\n","        count = 0\n","        for x, y in dataset.validation_dataset:\n","            state = None\n","            y_h, state = model(x.to(dev), state)\n","            loss = F.cross_entropy(y_h, y.to(dev))\n","            total_valid_loss += loss.item()\n","            total_valid_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","            count += 1\n","        average_valid_loss = total_valid_loss / count\n","        losses.append((average_train_loss, average_valid_loss))\n","        average_valid_accuracy = total_valid_correct / count\n","\n","\n","        print(f'epoch {epoch + 1} accuracies: \\t train: {average_train_accuracy}\\t valid: {average_valid_accuracy} loss: {average_train_loss}\\t')\n","\n","    # test mode\n","    model.eval()\n","    total_test_correct = 0\n","    count = 0\n","    for x, y in dataset.test_dataset:\n","        state = None\n","        y_h, state = model(x.to(dev), state)\n","        total_test_correct += (y_h.argmax(-1) == y.cuda()).float().mean()\n","        count += 1\n","    average_test_accuracy = total_test_correct / count\n","\n","    print(f'test accuracy {average_test_accuracy}')\n","\n","    return losses, (average_train_accuracy, average_valid_accuracy, average_test_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SbWvcYEurRyy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622962099350,"user_tz":-480,"elapsed":256098,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"aa3d12f6-c557-41de-a27f-ae433ace8691"},"source":["num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","k = 0\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","all_files = glob.glob(\"data/sentiment/*.csv\")\n","data = pd.concat((pd.read_csv(f, header=None, index_col=None) for f in all_files))\n","data.columns = ['sentence', 'label']\n","data = data.sample(frac=1, random_state=1)\n","dataset = DataLoader(data, k=k, batch_size = batch_size, context_window=context_window, dev=dev)\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[8 * dataset.no_batch, 14 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","# torch.save(model, os.path.join('./relexnet_model_negative/classifier_M7.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","Max Length : 68\n","Vocab Size : 5251\n","Modifier Size : 34\n","Negative Size : 24\n","K Fold Partition Completed!\n","K : 0, fold_data[k] : [0, 270]\n"," index : 76\n","Train Dataset Shape : torch.Size([2430, 68, 5253])\n","Validation Dataset Shape : torch.Size([270, 68, 5253])\n","Test Dataset Shape : torch.Size([300, 68, 5253])\n","Data Ready!\n","1 optim: 0.01\n","epoch 1 accuracies: \t train: 0.6157620549201965\t valid: 0.6884921193122864 loss: 0.6809743478109962\t\n","2 optim: 0.01\n","epoch 2 accuracies: \t train: 0.7864583134651184\t valid: 0.8070436716079712 loss: 0.6401133105943078\t\n","3 optim: 0.01\n","epoch 3 accuracies: \t train: 0.8168859481811523\t valid: 0.788690447807312 loss: 0.6069461852312088\t\n","4 optim: 0.01\n","epoch 4 accuracies: \t train: 0.8427358269691467\t valid: 0.8080357313156128 loss: 0.5818977293215299\t\n","5 optim: 0.01\n","epoch 5 accuracies: \t train: 0.8622258901596069\t valid: 0.824404776096344 loss: 0.5618216191467486\t\n","6 optim: 0.01\n","epoch 6 accuracies: \t train: 0.8621710538864136\t valid: 0.8373015522956848 loss: 0.5480828316588151\t\n","7 optim: 0.01\n","epoch 7 accuracies: \t train: 0.8811129331588745\t valid: 0.8209325671195984 loss: 0.5352972118478072\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.8844024538993835\t valid: 0.8358135223388672 loss: 0.5299603484178844\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.8897477984428406\t valid: 0.8179563283920288 loss: 0.5213940880800548\t\n","10 optim: 0.001\n","epoch 10 accuracies: \t train: 0.8942434191703796\t valid: 0.8303571343421936 loss: 0.5194115913227985\t\n","11 optim: 0.001\n","epoch 11 accuracies: \t train: 0.8905153870582581\t valid: 0.8358135223388672 loss: 0.5194965620574198\t\n","12 optim: 0.001\n","epoch 12 accuracies: \t train: 0.8930920958518982\t valid: 0.826884925365448 loss: 0.5185647885266104\t\n","13 optim: 0.001\n","epoch 13 accuracies: \t train: 0.8884868621826172\t valid: 0.8278769850730896 loss: 0.5205214207893923\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.8860746026039124\t valid: 0.8313491940498352 loss: 0.5170286015460366\t\n","15 optim: 0.0001\n","epoch 15 accuracies: \t train: 0.8992324471473694\t valid: 0.8313491940498352 loss: 0.5173934129507918\t\n","16 optim: 0.0001\n","epoch 16 accuracies: \t train: 0.8904879689216614\t valid: 0.8313491940498352 loss: 0.5182193360830608\t\n","17 optim: 0.0001\n","epoch 17 accuracies: \t train: 0.8966557383537292\t valid: 0.8313491940498352 loss: 0.5162228094904047\t\n","18 optim: 0.0001\n","epoch 18 accuracies: \t train: 0.9015899300575256\t valid: 0.8313491940498352 loss: 0.5130746976325387\t\n","19 optim: 0.0001\n","epoch 19 accuracies: \t train: 0.8963541984558105\t valid: 0.8134921193122864 loss: 0.516711207989015\t\n","20 optim: 0.0001\n","epoch 20 accuracies: \t train: 0.8971491456031799\t valid: 0.8358135223388672 loss: 0.5136535465717316\t\n","test accuracy 0.8333333134651184\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bnFCCwTmWQF2"},"source":["The code below is used for test on stance detection dataset before, \n","which is not used at all"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":292},"id":"-21dIPGEMbM-","executionInfo":{"status":"error","timestamp":1621401821354,"user_tz":-480,"elapsed":3446,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"9760b647-3bf9-4ddb-97cc-8daab4eaa4ed"},"source":["# Abortion\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","# root_path = 'data/stance'\n","# files = ['abortion', 'gayRights', 'marijuana', 'obama']\n","# label = {'abortion' : 0, 'gayRights' : 1, 'marijuana' : 2, 'obama' : 3}\n","# list_sentence = []\n","# list_label = []\n","# for file in files:\n","#     path = os.path.join('data', 'stance', file, '*.data')\n","#     print(path)\n","#     all_files = glob.glob(path)\n","#     # print(all_files)\n","#     for label_file in all_files:\n","#         f = open(label_file, 'r', encoding='UTF-8')\n","#         tmp = f.read()\n","#         list_sentence.append(tmp)\n","#         list_label.append(label[file])\n","\n","# dict = {'sentence': list_sentence, 'label': list_label}\n","# data = pd.DataFrame(dict)\n","# data = data.sample(frac=1, random_state=1)\n","\n","data = pd.read_csv(\"abortion.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=256)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, enc_negative=dataset.enc_negatives, negative_size=len(dataset.negative_vocab),modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_abortion.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","1915\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-0eec72c8ab4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# data = shuffle(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# print(f'Data Size: {dataset.df.size()}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Ready!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'k'"]}]},{"cell_type":"code","metadata":{"id":"bXeaQG7IQlVF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612431782072,"user_tz":-480,"elapsed":2148664,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"31e7b0a8-5317-4847-cb24-088879f72b3d"},"source":["# GayRights\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"gayRights.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, enc_negative=dataset.enc_negatives, negative_size=len(dataset.negative_vocab),modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_gayRights.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","1376\n","Max Length : 183\n","Vocab Size : 1591\n","Modifier Size : 34\n","Negative Modifier Size : 28\n"," index : 18\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([550, 183, 1593])\n","Input Label Shape : torch.Size([550])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([412, 183, 1593])\n","Input Label Shape : torch.Size([412])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([414, 183, 1593])\n","Input Label Shape : torch.Size([414])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.5983796119689941\t valid: 0.6286407709121704\t valid loss: 0.6714368381546539\t precision: 0.38714499252615847\t recall: 0.6286407766990292\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.6435185074806213\t valid: 0.6286407709121704\t valid loss: 0.6698871221646522\t precision: 0.38714499252615847\t recall: 0.6286407766990292\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.6435185074806213\t valid: 0.6286407709121704\t valid loss: 0.6691252636967353\t precision: 0.38714499252615847\t recall: 0.6286407766990292\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.6452546119689941\t valid: 0.6286407709121704\t valid loss: 0.6684725528347839\t precision: 0.38714499252615847\t recall: 0.6286407766990292\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.6226851940155029\t valid: 0.6286407709121704\t valid loss: 0.6678903915928406\t precision: 0.38714499252615847\t recall: 0.6286407766990292\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.6377314925193787\t valid: 0.6310679912567139\t valid loss: 0.6675301186957405\t precision: 0.38863976083707025\t recall: 0.6310679611650486\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.6284722089767456\t valid: 0.6310679912567139\t valid loss: 0.66707913418418\t precision: 0.38863976083707025\t recall: 0.6310679611650486\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.6510416865348816\t valid: 0.6286407709121704\t valid loss: 0.6669523911279382\t precision: 0.38830584707646176\t recall: 0.6286407766990292\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.6429398655891418\t valid: 0.6286407709121704\t valid loss: 0.666295739993887\t precision: 0.38830584707646176\t recall: 0.6286407766990292\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.6429398655891418\t valid: 0.6310679912567139\t valid loss: 0.6660765095532519\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.6371527910232544\t valid: 0.6310679912567139\t valid loss: 0.6660252697404149\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.6429398655891418\t valid: 0.6310679912567139\t valid loss: 0.6660382074927821\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.6516203880310059\t valid: 0.6310679912567139\t valid loss: 0.6659947898492072\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.6487268209457397\t valid: 0.6310679912567139\t valid loss: 0.6659133008238182\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.6412037014961243\t valid: 0.6310679912567139\t valid loss: 0.6658828409696088\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.6614583134651184\t valid: 0.6310679912567139\t valid loss: 0.665881003786638\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.6446759700775146\t valid: 0.6310679912567139\t valid loss: 0.6658708798943214\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.6649305820465088\t valid: 0.6310679912567139\t valid loss: 0.6658702629428466\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.6579861044883728\t valid: 0.6310679912567139\t valid loss: 0.6658651925695752\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.6579861044883728\t valid: 0.6310679912567139\t valid loss: 0.6658612253885825\t precision: 0.38980509745127434\t recall: 0.6310679611650486\t\n","test accuracy 0.6400966048240662 precision 0.39201183431952663 recall 0.6400966183574879\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZeMZ24XeFJd","executionInfo":{"status":"ok","timestamp":1612332424196,"user_tz":-480,"elapsed":473502,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"1c696722-468c-4c85-b0c8-908a5a2d7538"},"source":["# Marijuana\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"marijuana.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_marijuana.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","626\n","Max Length : 180\n","Vocab Size : 851\n","Modifier Size : 32\n"," index : 8\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([250, 180, 852])\n","Input Label Shape : torch.Size([250])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([187, 180, 852])\n","Input Label Shape : torch.Size([187])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([189, 180, 852])\n","Input Label Shape : torch.Size([189])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.25901442766189575\t valid: 0.29411765933036804\t valid loss: 0.7172110469583521\t precision: 0.9649122807017544\t recall: 0.29411764705882354\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.4624398946762085\t valid: 0.6631016135215759\t valid loss: 0.6803992186638124\t precision: 0.41471571906354515\t recall: 0.6631016042780749\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.7454928159713745\t valid: 0.7058823704719543\t valid loss: 0.6602817796130869\t precision: 0.416403785488959\t recall: 0.7058823529411765\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.7415865659713745\t valid: 0.7112299799919128\t valid loss: 0.650913705959677\t precision: 0.4169278996865204\t recall: 0.7112299465240641\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.7406851053237915\t valid: 0.7112299799919128\t valid loss: 0.6456264247868788\t precision: 0.4169278996865204\t recall: 0.7112299465240641\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.7385817170143127\t valid: 0.7058823704719543\t valid loss: 0.6421815856574054\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.7415865659713745\t valid: 0.7058823704719543\t valid loss: 0.6398314953169083\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.7433894276618958\t valid: 0.7058823704719543\t valid loss: 0.6381285659132157\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.6365765175398659\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.7445913553237915\t valid: 0.7058823704719543\t valid loss: 0.6353507488169134\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.7406851053237915\t valid: 0.7058823704719543\t valid loss: 0.6352448621216942\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.7379807829856873\t valid: 0.7058823704719543\t valid loss: 0.6351289008071715\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.7406851053237915\t valid: 0.7058823704719543\t valid loss: 0.6350228547412444\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.7406851053237915\t valid: 0.7058823704719543\t valid loss: 0.6349069501626938\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.7349759340286255\t valid: 0.7058823704719543\t valid loss: 0.63477713507127\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.7409855723381042\t valid: 0.7058823704719543\t valid loss: 0.634764927593782\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.7415865659713745\t valid: 0.7058823704719543\t valid loss: 0.6347521806464476\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.7427884340286255\t valid: 0.7058823704719543\t valid loss: 0.6347388900856283\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.7379807829856873\t valid: 0.7058823704719543\t valid loss: 0.6347260712621047\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.7436898946762085\t valid: 0.7058823704719543\t valid loss: 0.6347120630549875\t precision: 0.41379310344827586\t recall: 0.7058823529411765\t\n","test accuracy 0.6666666269302368 precision 0.4 recall 0.6666666666666666\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ul7Qg4IYgS9C","executionInfo":{"status":"ok","timestamp":1612337723036,"user_tz":-480,"elapsed":743295,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"3091891f-aa87-414e-f9cd-b29bf2dfb092"},"source":["# Obama\n","num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","\n","data = pd.read_csv(\"obama.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 2\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.001,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[10 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.1, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier_obama.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","985\n","Max Length : 176\n","Vocab Size : 1299\n","Modifier Size : 31\n"," index : 13\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([394, 176, 1300])\n","Input Label Shape : torch.Size([394])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([295, 176, 1300])\n","Input Label Shape : torch.Size([295])\n","Input Data Shape (sequence_num, sequence_len, vocab_size + 1) : torch.Size([296, 176, 1300])\n","Input Label Shape : torch.Size([296])\n","Data Ready!\n","1 optim: 0.001\n","epoch 1 accuracies: \t train: 0.523557722568512\t valid: 0.5559322237968445\t valid loss: 0.6914822905750598\t precision: 0.3685393258426966\t recall: 0.5559322033898305\t\n","2 optim: 0.001\n","epoch 2 accuracies: \t train: 0.5490384697914124\t valid: 0.5186440348625183\t valid loss: 0.6923747814307779\t precision: 0.7320574162679426\t recall: 0.5186440677966102\t\n","3 optim: 0.001\n","epoch 3 accuracies: \t train: 0.5918269157409668\t valid: 0.5186440348625183\t valid loss: 0.6914110822192693\t precision: 0.6923076923076923\t recall: 0.5186440677966102\t\n","4 optim: 0.001\n","epoch 4 accuracies: \t train: 0.6990384459495544\t valid: 0.5694915056228638\t valid loss: 0.6892165515382411\t precision: 0.4528301886792453\t recall: 0.5694915254237288\t\n","5 optim: 0.001\n","epoch 5 accuracies: \t train: 0.6639423370361328\t valid: 0.5661016702651978\t valid loss: 0.6884974831241673\t precision: 0.4123456790123457\t recall: 0.5661016949152542\t\n","6 optim: 0.001\n","epoch 6 accuracies: \t train: 0.7096154689788818\t valid: 0.5966101884841919\t valid loss: 0.6883900100901975\t precision: 0.5623003194888179\t recall: 0.5966101694915255\t\n","7 optim: 0.001\n","epoch 7 accuracies: \t train: 0.7807692289352417\t valid: 0.5932203531265259\t valid loss: 0.6882448610612901\t precision: 0.6097560975609756\t recall: 0.5932203389830508\t\n","8 optim: 0.001\n","epoch 8 accuracies: \t train: 0.7485576868057251\t valid: 0.5966101884841919\t valid loss: 0.6869916800725258\t precision: 0.5587301587301587\t recall: 0.5966101694915255\t\n","9 optim: 0.001\n","epoch 9 accuracies: \t train: 0.7774038314819336\t valid: 0.5932203531265259\t valid loss: 0.6863108917818231\t precision: 0.541795665634675\t recall: 0.5932203389830508\t\n","10 optim: 0.0001\n","epoch 10 accuracies: \t train: 0.7846153974533081\t valid: 0.5932203531265259\t valid loss: 0.6856528217509642\t precision: 0.5351681957186545\t recall: 0.5932203389830508\t\n","11 optim: 0.0001\n","epoch 11 accuracies: \t train: 0.7802885174751282\t valid: 0.5999999642372131\t valid loss: 0.685643940456843\t precision: 0.5412844036697247\t recall: 0.6\t\n","12 optim: 0.0001\n","epoch 12 accuracies: \t train: 0.8216345906257629\t valid: 0.5999999642372131\t valid loss: 0.6856010727963205\t precision: 0.5412844036697247\t recall: 0.6\t\n","13 optim: 0.0001\n","epoch 13 accuracies: \t train: 0.7745192646980286\t valid: 0.5966101884841919\t valid loss: 0.6856043407472513\t precision: 0.5415384615384615\t recall: 0.5966101694915255\t\n","14 optim: 0.0001\n","epoch 14 accuracies: \t train: 0.8211538791656494\t valid: 0.5966101884841919\t valid loss: 0.6855390968969313\t precision: 0.5415384615384615\t recall: 0.5966101694915255\t\n","15 optim: 1e-05\n","epoch 15 accuracies: \t train: 0.8384615182876587\t valid: 0.5999999642372131\t valid loss: 0.6855586565147012\t precision: 0.5446153846153846\t recall: 0.6\t\n","16 optim: 1e-05\n","epoch 16 accuracies: \t train: 0.8125000596046448\t valid: 0.5999999642372131\t valid loss: 0.6855438086946132\t precision: 0.5446153846153846\t recall: 0.6\t\n","17 optim: 1e-05\n","epoch 17 accuracies: \t train: 0.7947115898132324\t valid: 0.5999999642372131\t valid loss: 0.6855282175338875\t precision: 0.5446153846153846\t recall: 0.6\t\n","18 optim: 1e-05\n","epoch 18 accuracies: \t train: 0.8134615421295166\t valid: 0.5966101884841919\t valid loss: 0.6855167114128501\t precision: 0.5415384615384615\t recall: 0.5966101694915255\t\n","19 optim: 1e-05\n","epoch 19 accuracies: \t train: 0.7927884459495544\t valid: 0.5999999642372131\t valid loss: 0.6855150931972568\t precision: 0.5446153846153846\t recall: 0.6\t\n","20 optim: 1e-05\n","epoch 20 accuracies: \t train: 0.7870192527770996\t valid: 0.5999999642372131\t valid loss: 0.6855109792644695\t precision: 0.5446153846153846\t recall: 0.6\t\n","test accuracy 0.6148648858070374 precision 0.5777777777777777 recall 0.6148648648648649\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":526},"id":"vnSy7vADC0kK","executionInfo":{"status":"error","timestamp":1612420266396,"user_tz":-480,"elapsed":11573,"user":{"displayName":"刘世萱","photoUrl":"","userId":"15293057863655146379"}},"outputId":"55951f28-644f-457c-8765-a8f3372b30ca"},"source":["num_epochs = 20\n","batch_size = 32  # Number of hidden neurons in model\n","context_window = 1\n","\n","dev = 'cuda' if torch.cuda.is_available() else 'cpu'  # If you have a GPU installed, use that, otherwise CPU\n","print(dev)\n","print('Loading data...')\n","# root_path = 'data/stance'\n","# files = ['abortion', 'gayRights', 'marijuana', 'obama']\n","# label = {'abortion' : 0, 'gayRights' : 1, 'marijuana' : 2, 'obama' : 3}\n","# list_sentence = []\n","# list_label = []\n","# for file in files:\n","#     path = os.path.join('data', 'stance', file, '*.data')\n","#     print(path)\n","#     all_files = glob.glob(path)\n","#     # print(all_files)\n","#     for label_file in all_files:\n","#         f = open(label_file, 'r', encoding='UTF-8')\n","#         tmp = f.read()\n","#         list_sentence.append(tmp)\n","#         list_label.append(label[file])\n","\n","# dict = {'sentence': list_sentence, 'label': list_label}\n","# data = pd.DataFrame(dict)\n","# data = data.sample(frac=1, random_state=1)\n","\n","data = pd.read_csv(\"stance_dataset.csv\", index_col=None, error_bad_lines=False, encoding='UTF-8')\n","print(len(data))\n","# print(data)\n","data = data.sample(frac=1, random_state=1)\n","# data = shuffle(data)\n","dataset = DataLoader(data, batch_size = batch_size, context_window=context_window, dev=dev)\n","# print(f'Data Size: {dataset.df.size()}')\n","print(\"Data Ready!\")\n","vocabulary_size = len(dataset.vocab)\n","len_seq = dataset.max_length\n","num_classes = 4\n","\n","# model = FastText(len(dataset.token_to_id)+2, num_hidden, len(dataset.class_to_id)).to(dev)\n","model = RelexNet(vocabulary_size=vocabulary_size, enc=dataset.enc_modifiers, modifier_size=len(dataset.modifier_vocab),len_seq=len_seq, num_classes=num_classes).cuda()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","optimizer = torch.optim.Adam(model.parameters(),lr=0.005,betas=(0.9,0.99))\n","# optimizer = torch.optim.Adam(model.parameters(),lr=0.01,betas=(0.9,0.99), weight_decay=10)\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[3 * dataset.no_batch, 15 * dataset.no_batch],gamma = 0.9, last_epoch=-1)\n","\n","losses, accuracies = train_model(dataset, model, optimizer, scheduler, num_epochs, dev=dev)\n","torch.save(model, os.path.join('saved_models', 'classifier2.pth'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n","Loading data...\n","4902\n","Max Length : 189\n","Vocab Size : 3949\n","Modifier Size : 43\n","Negative Modifier Size : 29\n"," index : 62\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-ff746ca97aca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# data = shuffle(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_window\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;31m# print(f'Data Size: {dataset.df.size()}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Ready!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, batch_size, context_window, preprocessor, enc_tokens, enc_modifiers, enc_negative_modifiers, modifier, dev)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(len(self.test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36mbuild_training_data\u001b[0;34m(self, enc, df)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_modifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-4ccd9043e5f0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiers\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegatives\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_modifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;31m# validation of X happens in _check_X called by _transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mX_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_unknown\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_unknown\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_int\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, handle_unknown)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mXi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],\n\u001b[0;32m--> 118\u001b[0;31m                                                      return_mask=True)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[0;34m(values, uniques, return_mask)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \"\"\"\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0muniques_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muniques_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"cgJa4WP1rkU4"},"source":["torch.save(model, os.path.join('classifier.pth'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rdJa617xw7N3"},"source":["!pip install kora\n","from kora import console\n","console.start()  # and click link"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzzcdPQuRcyw"},"source":[""],"execution_count":null,"outputs":[]}]}